{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f4bd475-7b7e-4bf0-bfec-f5bc5ad4fddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=5000, dim=5\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 5000 edges.\n",
      "[INFO] Train relations: 4000\n",
      "[INFO] Test relations : 1000\n",
      "[INFO] Training Poincaré (dim=5, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=5000, dim=10\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 5000 edges.\n",
      "[INFO] Train relations: 4000\n",
      "[INFO] Test relations : 1000\n",
      "[INFO] Training Poincaré (dim=10, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=5000, dim=20\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 5000 edges.\n",
      "[INFO] Train relations: 4000\n",
      "[INFO] Test relations : 1000\n",
      "[INFO] Training Poincaré (dim=20, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=5000, dim=50\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 5000 edges.\n",
      "[INFO] Train relations: 4000\n",
      "[INFO] Test relations : 1000\n",
      "[INFO] Training Poincaré (dim=50, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=5000, dim=100\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 5000 edges.\n",
      "[INFO] Train relations: 4000\n",
      "[INFO] Test relations : 1000\n",
      "[INFO] Training Poincaré (dim=100, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=5000, dim=200\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 5000 edges.\n",
      "[INFO] Train relations: 4000\n",
      "[INFO] Test relations : 1000\n",
      "[INFO] Training Poincaré (dim=200, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=10000, dim=5\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 10000 edges.\n",
      "[INFO] Train relations: 8000\n",
      "[INFO] Test relations : 2000\n",
      "[INFO] Training Poincaré (dim=5, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=10000, dim=10\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 10000 edges.\n",
      "[INFO] Train relations: 8000\n",
      "[INFO] Test relations : 2000\n",
      "[INFO] Training Poincaré (dim=10, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=10000, dim=20\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 10000 edges.\n",
      "[INFO] Train relations: 8000\n",
      "[INFO] Test relations : 2000\n",
      "[INFO] Training Poincaré (dim=20, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=10000, dim=50\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 10000 edges.\n",
      "[INFO] Train relations: 8000\n",
      "[INFO] Test relations : 2000\n",
      "[INFO] Training Poincaré (dim=50, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=10000, dim=100\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 10000 edges.\n",
      "[INFO] Train relations: 8000\n",
      "[INFO] Test relations : 2000\n",
      "[INFO] Training Poincaré (dim=100, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=10000, dim=200\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 10000 edges.\n",
      "[INFO] Train relations: 8000\n",
      "[INFO] Test relations : 2000\n",
      "[INFO] Training Poincaré (dim=200, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=15000, dim=5\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 15000 edges.\n",
      "[INFO] Train relations: 12000\n",
      "[INFO] Test relations : 3000\n",
      "[INFO] Training Poincaré (dim=5, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=15000, dim=10\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 15000 edges.\n",
      "[INFO] Train relations: 12000\n",
      "[INFO] Test relations : 3000\n",
      "[INFO] Training Poincaré (dim=10, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=15000, dim=20\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 15000 edges.\n",
      "[INFO] Train relations: 12000\n",
      "[INFO] Test relations : 3000\n",
      "[INFO] Training Poincaré (dim=20, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=15000, dim=50\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 15000 edges.\n",
      "[INFO] Train relations: 12000\n",
      "[INFO] Test relations : 3000\n",
      "[INFO] Training Poincaré (dim=50, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=15000, dim=100\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 15000 edges.\n",
      "[INFO] Train relations: 12000\n",
      "[INFO] Test relations : 3000\n",
      "[INFO] Training Poincaré (dim=100, epochs=100) ...\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=15000, dim=200\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_combined.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 15000 edges.\n",
      "[INFO] Train relations: 12000\n",
      "[INFO] Test relations : 3000\n",
      "[INFO] Training Poincaré (dim=200, epochs=100) ...\n",
      "\n",
      "[INFO] All experiment results saved to 'facebook_experiments.xlsx'\n",
      "\n",
      "All experiment runs:\n",
      "     subset_size  embedding_dim  test_size  epochs  n_negatives_strict  \\\n",
      "0          5000              5        0.2     100                  50   \n",
      "1          5000             10        0.2     100                  50   \n",
      "2          5000             20        0.2     100                  50   \n",
      "3          5000             50        0.2     100                  50   \n",
      "4          5000            100        0.2     100                  50   \n",
      "5          5000            200        0.2     100                  50   \n",
      "6         10000              5        0.2     100                  50   \n",
      "7         10000             10        0.2     100                  50   \n",
      "8         10000             20        0.2     100                  50   \n",
      "9         10000             50        0.2     100                  50   \n",
      "10        10000            100        0.2     100                  50   \n",
      "11        10000            200        0.2     100                  50   \n",
      "12        15000              5        0.2     100                  50   \n",
      "13        15000             10        0.2     100                  50   \n",
      "14        15000             20        0.2     100                  50   \n",
      "15        15000             50        0.2     100                  50   \n",
      "16        15000            100        0.2     100                  50   \n",
      "17        15000            200        0.2     100                  50   \n",
      "\n",
      "    recon_mean_rank_strict  recon_map_strict  lp_mean_rank    lp_map  \\\n",
      "0                 1.421000          0.881696      7.149677  0.454991   \n",
      "1                 1.332250          0.899576      6.828387  0.459273   \n",
      "2                 1.316000          0.905673      6.722581  0.463314   \n",
      "3                 1.296000          0.911405      6.583226  0.465704   \n",
      "4                 1.290500          0.914628      6.636129  0.464873   \n",
      "5                 1.295500          0.915539      6.741935  0.461610   \n",
      "6                 1.654625          0.818153      3.817981  0.577174   \n",
      "7                 1.630625          0.829276      3.913955  0.584261   \n",
      "8                 1.605625          0.836521      3.777716  0.594422   \n",
      "9                 1.598500          0.838326      3.817981  0.593229   \n",
      "10                1.579750          0.840458      3.800883  0.596752   \n",
      "11                1.591250          0.837958      3.860452  0.597511   \n",
      "12                1.847333          0.789760      3.262221  0.626887   \n",
      "13                1.765750          0.799837      3.216830  0.638619   \n",
      "14                1.781167          0.801970      3.271299  0.630810   \n",
      "15                1.749833          0.804401      3.205307  0.634704   \n",
      "16                1.745250          0.806340      3.262570  0.637775   \n",
      "17                1.764333          0.800957      3.303073  0.632478   \n",
      "\n",
      "    lp_precision_10  lp_recall_10  \n",
      "0          0.805161      0.805161  \n",
      "1          0.829677      0.829677  \n",
      "2          0.836129      0.836129  \n",
      "3          0.833548      0.833548  \n",
      "4          0.827097      0.827097  \n",
      "5          0.825806      0.825806  \n",
      "6          0.928847      0.928847  \n",
      "7          0.927192      0.927192  \n",
      "8          0.933260      0.933260  \n",
      "9          0.929950      0.929950  \n",
      "10         0.931054      0.931054  \n",
      "11         0.930502      0.930502  \n",
      "12         0.953911      0.953911  \n",
      "13         0.952863      0.952863  \n",
      "14         0.953911      0.953911  \n",
      "15         0.954958      0.954958  \n",
      "16         0.953212      0.953212  \n",
      "17         0.952514      0.952514  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models.poincare import PoincareModel\n",
    "\n",
    "\n",
    "\n",
    "def load_facebook_edges(\n",
    "    filepath,      \n",
    "    subset_size=None, \n",
    "    test_size=0.2,\n",
    "    seed=42\n",
    "):\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Edge list file not found: {filepath}\")\n",
    "\n",
    "    print(f\"[INFO] Loading edges from: {filepath} ...\")\n",
    "    edges = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            \n",
    "            u, v = parts[0], parts[1]\n",
    "\n",
    "\n",
    "            edges.append((u, v))\n",
    "\n",
    "    print(f\"[INFO] Total edges loaded: {len(edges)}\")\n",
    "\n",
    "    \n",
    "    if subset_size is not None and len(edges) > subset_size:\n",
    "        edges = random.sample(edges, subset_size)\n",
    "        print(f\"[INFO] Using a SUBSET of {subset_size} edges.\")\n",
    "\n",
    "    \n",
    "    train_relations, test_relations = train_test_split(\n",
    "        edges, test_size=test_size, random_state=seed\n",
    "    )\n",
    "    print(f\"[INFO] Train relations: {len(train_relations)}\")\n",
    "    print(f\"[INFO] Test relations : {len(test_relations)}\")\n",
    "\n",
    "    combined_relations = train_relations + test_relations\n",
    "    return train_relations, test_relations, combined_relations\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_poincare(\n",
    "    train_relations,\n",
    "    test_relations,\n",
    "    combined_relations,\n",
    "    embedding_dim=10,\n",
    "    epochs=50,\n",
    "    n_negatives_strict=500,\n",
    "):\n",
    "\n",
    "    print(f\"[INFO] Training Poincaré (dim={embedding_dim}, epochs={epochs}) ...\")\n",
    "    model = PoincareModel(\n",
    "        train_data=train_relations,\n",
    "        size=embedding_dim,\n",
    "        negative=10,  \n",
    "        burn_in=10    \n",
    "    )\n",
    "    model.train(epochs=epochs)\n",
    "\n",
    "    \n",
    "    u_to_all_neighbors = defaultdict(set)\n",
    "    for (u, v) in combined_relations:\n",
    "        u_to_all_neighbors[u].add(v)\n",
    "\n",
    "    vocab_nodes = set(model.kv.index_to_key)\n",
    "    vocab_list = list(vocab_nodes)\n",
    "    all_edges_set = set(combined_relations)\n",
    "\n",
    "    \n",
    "    recon_mr_strict = reconstruction_mean_rank_strict_sampled(\n",
    "        model, train_relations, u_to_all_neighbors, vocab_list, n_negatives_strict\n",
    "    )\n",
    "    recon_map_strict = reconstruction_map_strict_sampled(\n",
    "        model, train_relations, u_to_all_neighbors, vocab_list, n_negatives_strict\n",
    "    )\n",
    "\n",
    "    \n",
    "    lp_mr = link_prediction_mean_rank(model, test_relations, vocab_list, all_edges_set)\n",
    "    lp_map_ = link_prediction_map(model, test_relations, vocab_list, all_edges_set)\n",
    "    lp_p10 = precision_at_k(model, test_relations, vocab_list, all_edges_set, k=10)\n",
    "    lp_r10 = recall_at_k(model, test_relations, vocab_list, all_edges_set, k=10)\n",
    "\n",
    "    return {\n",
    "        \"recon_mean_rank_strict\": recon_mr_strict,\n",
    "        \"recon_map_strict\": recon_map_strict,\n",
    "        \"lp_mean_rank\": lp_mr,\n",
    "        \"lp_map\": lp_map_,\n",
    "        \"lp_precision_10\": lp_p10,\n",
    "        \"lp_recall_10\": lp_r10,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def reconstruction_mean_rank_strict_sampled(model, edges, u_to_all_neighbors, vocab_list, n_negatives=500, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    ranks = []\n",
    "    for (u, v) in edges:\n",
    "        if (u not in model.kv) or (v not in model.kv):\n",
    "            continue\n",
    "\n",
    "        neighbors = u_to_all_neighbors[u]\n",
    "        neg_candidates = []\n",
    "        attempts = 0\n",
    "        while len(neg_candidates) < n_negatives and attempts < 10000:\n",
    "            candidate = rng.choice(vocab_list)\n",
    "            if candidate not in (u, v) and candidate not in neighbors:\n",
    "                neg_candidates.append(candidate)\n",
    "            attempts += 1\n",
    "\n",
    "        candidates = neg_candidates + [v]\n",
    "        dists = [(c, model.kv.distance(u, c)) for c in candidates]\n",
    "        sorted_nodes = [x[0] for x in sorted(dists, key=lambda x: x[1])]\n",
    "\n",
    "        try:\n",
    "            rank = sorted_nodes.index(v) + 1\n",
    "            ranks.append(rank)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return float(np.mean(ranks)) if ranks else 0.0\n",
    "\n",
    "\n",
    "def reconstruction_map_strict_sampled(model, edges, u_to_all_neighbors, vocab_list, n_negatives=500, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    reciprocal_ranks = []\n",
    "    for (u, v) in edges:\n",
    "        if (u not in model.kv) or (v not in model.kv):\n",
    "            continue\n",
    "\n",
    "        neighbors = u_to_all_neighbors[u]\n",
    "        neg_candidates = []\n",
    "        attempts = 0\n",
    "        while len(neg_candidates) < n_negatives and attempts < 10000:\n",
    "            candidate = rng.choice(vocab_list)\n",
    "            if candidate not in (u, v) and (candidate not in neighbors):\n",
    "                neg_candidates.append(candidate)\n",
    "            attempts += 1\n",
    "\n",
    "        candidates = neg_candidates + [v]\n",
    "        dists = [(c, model.kv.distance(u, c)) for c in candidates]\n",
    "        sorted_nodes = [x[0] for x in sorted(dists, key=lambda x: x[1])]\n",
    "        try:\n",
    "            r = sorted_nodes.index(v) + 1\n",
    "            reciprocal_ranks.append(1.0 / r)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return float(np.mean(reciprocal_ranks)) if reciprocal_ranks else 0.0\n",
    "\n",
    "\n",
    "def link_prediction_mean_rank(model, edges, vocab_list, all_edges_set, num_negatives=50):\n",
    "    rng = np.random.default_rng(1234)\n",
    "    ranks = []\n",
    "    for (source, target) in edges:\n",
    "        if (source not in model.kv) or (target not in model.kv):\n",
    "            continue\n",
    "\n",
    "        neg_candidates = []\n",
    "        attempts = 0\n",
    "        while len(neg_candidates) < num_negatives and attempts < 10000:\n",
    "            candidate = rng.choice(vocab_list)\n",
    "            if (candidate != target) and ((source, candidate) not in all_edges_set):\n",
    "                neg_candidates.append(candidate)\n",
    "            attempts += 1\n",
    "\n",
    "        if not neg_candidates:\n",
    "            continue\n",
    "\n",
    "        candidates = neg_candidates + [target]\n",
    "        dists = [model.kv.distance(source, c) for c in candidates]\n",
    "        sorted_candidates = [c for _, c in sorted(zip(dists, candidates), key=lambda x: x[0])]\n",
    "        rank = sorted_candidates.index(target) + 1\n",
    "        ranks.append(rank)\n",
    "\n",
    "    return float(np.mean(ranks)) if ranks else 0.0\n",
    "\n",
    "\n",
    "def link_prediction_map(model, edges, vocab_list, all_edges_set, num_negatives=50):\n",
    "    rng = np.random.default_rng(1234)\n",
    "    reciprocal_ranks = []\n",
    "    for (source, target) in edges:\n",
    "        if (source not in model.kv) or (target not in model.kv):\n",
    "            continue\n",
    "\n",
    "        neg_candidates = []\n",
    "        attempts = 0\n",
    "        while len(neg_candidates) < num_negatives and attempts < 10000:\n",
    "            candidate = rng.choice(vocab_list)\n",
    "            if (candidate != target) and ((source, candidate) not in all_edges_set):\n",
    "                neg_candidates.append(candidate)\n",
    "            attempts += 1\n",
    "\n",
    "        if not neg_candidates:\n",
    "            continue\n",
    "\n",
    "        candidates = neg_candidates + [target]\n",
    "        dists = [model.kv.distance(source, c) for c in candidates]\n",
    "        sorted_candidates = [c for _, c in sorted(zip(dists, candidates), key=lambda x: x[0])]\n",
    "        rank = sorted_candidates.index(target) + 1\n",
    "        reciprocal_ranks.append(1.0 / rank)\n",
    "\n",
    "    return float(np.mean(reciprocal_ranks)) if reciprocal_ranks else 0.0\n",
    "\n",
    "\n",
    "def precision_at_k(model, edges, vocab_list, all_edges_set, k=10, num_negatives=50):\n",
    "    rng = np.random.default_rng(1234)\n",
    "    hits = 0\n",
    "    count = 0\n",
    "    for (source, target) in edges:\n",
    "        if (source not in model.kv) or (target not in model.kv):\n",
    "            continue\n",
    "\n",
    "        neg_candidates = []\n",
    "        attempts = 0\n",
    "        while len(neg_candidates) < num_negatives and attempts < 10000:\n",
    "            candidate = rng.choice(vocab_list)\n",
    "            if (candidate != target) and ((source, candidate) not in all_edges_set):\n",
    "                neg_candidates.append(candidate)\n",
    "            attempts += 1\n",
    "\n",
    "        if not neg_candidates:\n",
    "            continue\n",
    "\n",
    "        candidates = neg_candidates + [target]\n",
    "        dists = [model.kv.distance(source, c) for c in candidates]\n",
    "        sorted_candidates = [c for _, c in sorted(zip(dists, candidates), key=lambda x: x[0])]\n",
    "        top_k_nodes = sorted_candidates[:k]\n",
    "\n",
    "        if target in top_k_nodes:\n",
    "            hits += 1\n",
    "        count += 1\n",
    "\n",
    "    return hits / count if count else 0.0\n",
    "\n",
    "\n",
    "def recall_at_k(model, edges, vocab_list, all_edges_set, k=10, num_negatives=50):\n",
    "    \n",
    "    return precision_at_k(model, edges, vocab_list, all_edges_set, k, num_negatives)\n",
    "\n",
    "\n",
    "def run_facebook_pipeline(\n",
    "    edge_file=\"facebook_edges.txt\",\n",
    "    subset_size=10000,\n",
    "    test_size=0.2,\n",
    "    embedding_dim=10,\n",
    "    epochs=50,\n",
    "    n_negatives_strict=500,\n",
    "    excel_output=\"facebook_poincare_results.xlsx\"\n",
    "):\n",
    "\n",
    "    train_rel, test_rel, combined_rel = load_facebook_edges(\n",
    "        filepath=edge_file,\n",
    "        subset_size=subset_size,\n",
    "        test_size=test_size,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    results = train_and_evaluate_poincare(\n",
    "        train_relations=train_rel,\n",
    "        test_relations=test_rel,\n",
    "        combined_relations=combined_rel,\n",
    "        embedding_dim=embedding_dim,\n",
    "        epochs=epochs,\n",
    "        n_negatives_strict=n_negatives_strict\n",
    "    )\n",
    "\n",
    "    # Save in a single-row DataFrame\n",
    "    df = pd.DataFrame([{\n",
    "        \"edge_file\": edge_file,\n",
    "        \"subset_size\": subset_size,\n",
    "        \"test_size\": test_size,\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"epochs\": epochs,\n",
    "        \"n_negatives_strict\": n_negatives_strict,\n",
    "        **results\n",
    "    }])\n",
    "    df.to_excel(excel_output, index=False)\n",
    "    print(f\"\\n[INFO] Results saved to: {excel_output}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def run_multiple_experiments(\n",
    "    edge_file=\"facebook_edges.txt\",\n",
    "    subset_sizes=[1000, 5000, 10000],      \n",
    "    dims=[5, 10, 20],                     \n",
    "    test_size=0.2,\n",
    "    epochs=50,\n",
    "    n_negatives_strict=100,\n",
    "    excel_output=\"facebook_experiments.xlsx\"\n",
    "):\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for s_size in subset_sizes:\n",
    "        for dim in dims:\n",
    "            print(\"========================================================\")\n",
    "            print(f\"RUNNING EXPERIMENT: subset_size={s_size}, dim={dim}\")\n",
    "            print(\"========================================================\")\n",
    "\n",
    "            \n",
    "            train_rel, test_rel, combined_rel = load_facebook_edges(\n",
    "                filepath=edge_file,\n",
    "                subset_size=s_size,\n",
    "                test_size=test_size,\n",
    "                seed=42\n",
    "            )\n",
    "\n",
    "            \n",
    "            results = train_and_evaluate_poincare(\n",
    "                train_relations=train_rel,\n",
    "                test_relations=test_rel,\n",
    "                combined_relations=combined_rel,\n",
    "                embedding_dim=dim,\n",
    "                epochs=epochs,\n",
    "                n_negatives_strict=n_negatives_strict\n",
    "            )\n",
    "\n",
    "           \n",
    "            row_info = {\n",
    "                \"subset_size\": s_size,\n",
    "                \"embedding_dim\": dim,\n",
    "                \"test_size\": test_size,\n",
    "                \"epochs\": epochs,\n",
    "                \"n_negatives_strict\": n_negatives_strict\n",
    "            }\n",
    "            row_info.update(results)\n",
    "            all_results.append(row_info)\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_excel(excel_output, index=False)\n",
    "    print(f\"\\n[INFO] All experiment results saved to '{excel_output}'\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    desktop_path = os.path.expanduser(\"~/Desktop/experiment_results_facebook.xlsx\") \n",
    "    \n",
    "    # MULTIPLE EXPERIMENTS:\n",
    "    final_multi = run_multiple_experiments(\n",
    "        edge_file = os.path.expanduser(\"~/Desktop/facebook_combined.txt\"),\n",
    "        subset_sizes=[5000, 10000, 15000],  \n",
    "        dims=[5, 10,20,50,100,200] ,             \n",
    "        test_size=0.2,\n",
    "        epochs=100,                        \n",
    "        n_negatives_strict=50,\n",
    "        excel_output=\"facebook_experiments.xlsx\"\n",
    "    )\n",
    "    print(\"\\nAll experiment runs:\\n\", final_multi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d16362d-a3ed-4689-8fd0-5f7b3af93a99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
