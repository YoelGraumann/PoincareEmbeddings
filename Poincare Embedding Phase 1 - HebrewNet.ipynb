{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7440d7ae-e7e1-4c52-abba-a040b331e4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total edges (before subsampling): 196\n",
      "Train relations: 156\n",
      "Test relations : 40\n",
      "========================================================\n",
      "RUNNING: dim=5, removal_prob=0.00\n",
      "========================================================\n",
      "Removed 0 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=5, removal_prob=0.10\n",
      "========================================================\n",
      "Removed 19 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=5, removal_prob=0.30\n",
      "========================================================\n",
      "Removed 54 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=10, removal_prob=0.00\n",
      "========================================================\n",
      "Removed 0 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=10, removal_prob=0.10\n",
      "========================================================\n",
      "Removed 19 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=10, removal_prob=0.30\n",
      "========================================================\n",
      "Removed 54 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=20, removal_prob=0.00\n",
      "========================================================\n",
      "Removed 0 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=20, removal_prob=0.10\n",
      "========================================================\n",
      "Removed 19 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=20, removal_prob=0.30\n",
      "========================================================\n",
      "Removed 54 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=50, removal_prob=0.00\n",
      "========================================================\n",
      "Removed 0 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=50, removal_prob=0.10\n",
      "========================================================\n",
      "Removed 19 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=50, removal_prob=0.30\n",
      "========================================================\n",
      "Removed 54 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=100, removal_prob=0.00\n",
      "========================================================\n",
      "Removed 0 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=100, removal_prob=0.10\n",
      "========================================================\n",
      "Removed 19 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=100, removal_prob=0.30\n",
      "========================================================\n",
      "Removed 54 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=200, removal_prob=0.00\n",
      "========================================================\n",
      "Removed 0 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=200, removal_prob=0.10\n",
      "========================================================\n",
      "Removed 19 edges from training set.\n",
      "========================================================\n",
      "RUNNING: dim=200, removal_prob=0.30\n",
      "========================================================\n",
      "Removed 54 edges from training set.\n",
      "\n",
      "All experiment results saved to: Y:\\Data Science Readings\\Geometry of Information\\hebrew\\hebrewnet_experiment.xlsx\n",
      "\n",
      "Final DataFrame of all runs:\n",
      "\n",
      "    embedding_dim  edge_removal_probability  subset_size  test_size  epochs  \\\n",
      "0               5                       0.0        10000        0.2     100   \n",
      "1               5                       0.1        10000        0.2     100   \n",
      "2               5                       0.3        10000        0.2     100   \n",
      "3              10                       0.0        10000        0.2     100   \n",
      "4              10                       0.1        10000        0.2     100   \n",
      "5              10                       0.3        10000        0.2     100   \n",
      "6              20                       0.0        10000        0.2     100   \n",
      "7              20                       0.1        10000        0.2     100   \n",
      "8              20                       0.3        10000        0.2     100   \n",
      "9              50                       0.0        10000        0.2     100   \n",
      "10             50                       0.1        10000        0.2     100   \n",
      "11             50                       0.3        10000        0.2     100   \n",
      "12            100                       0.0        10000        0.2     100   \n",
      "13            100                       0.1        10000        0.2     100   \n",
      "14            100                       0.3        10000        0.2     100   \n",
      "15            200                       0.0        10000        0.2     100   \n",
      "16            200                       0.1        10000        0.2     100   \n",
      "17            200                       0.3        10000        0.2     100   \n",
      "\n",
      "    n_negatives_strict  reconstruction_mean_rank_strict  \\\n",
      "0                   20                         1.179487   \n",
      "1                   20                         1.248175   \n",
      "2                   20                         1.235294   \n",
      "3                   20                         1.211538   \n",
      "4                   20                         1.299270   \n",
      "5                   20                         1.196078   \n",
      "6                   20                         1.256410   \n",
      "7                   20                         1.291971   \n",
      "8                   20                         1.196078   \n",
      "9                   20                         1.217949   \n",
      "10                  20                         1.255474   \n",
      "11                  20                         1.235294   \n",
      "12                  20                         1.185897   \n",
      "13                  20                         1.248175   \n",
      "14                  20                         1.215686   \n",
      "15                  20                         1.230769   \n",
      "16                  20                         1.248175   \n",
      "17                  20                         1.176471   \n",
      "\n",
      "    reconstruction_map_strict  lp_mean_rank    lp_map  lp_precision_10  \\\n",
      "0                    0.923077        14.625  0.287247            0.625   \n",
      "1                    0.895985        17.250  0.090795            0.500   \n",
      "2                    0.899510        17.250  0.080191            0.250   \n",
      "3                    0.909722        11.125  0.275124            0.625   \n",
      "4                    0.875912        15.500  0.128338            0.625   \n",
      "5                    0.911765        14.375  0.152099            0.750   \n",
      "6                    0.891560        14.625  0.318570            0.625   \n",
      "7                    0.881995        13.625  0.265971            0.750   \n",
      "8                    0.911765        15.750  0.117518            0.625   \n",
      "9                    0.910256        10.500  0.262740            0.625   \n",
      "10                   0.889294        12.750  0.242336            0.625   \n",
      "11                   0.898693        17.375  0.125189            0.500   \n",
      "12                   0.924679        13.375  0.319852            0.625   \n",
      "13                   0.899635        14.625  0.276677            0.625   \n",
      "14                   0.912582        12.750  0.159483            0.500   \n",
      "15                   0.904380         8.625  0.288485            0.625   \n",
      "16                   0.896837        13.625  0.261258            0.625   \n",
      "17                   0.921569        16.500  0.147097            0.500   \n",
      "\n",
      "    lp_recall_10  \n",
      "0          0.625  \n",
      "1          0.500  \n",
      "2          0.250  \n",
      "3          0.625  \n",
      "4          0.625  \n",
      "5          0.750  \n",
      "6          0.625  \n",
      "7          0.750  \n",
      "8          0.625  \n",
      "9          0.625  \n",
      "10         0.625  \n",
      "11         0.500  \n",
      "12         0.625  \n",
      "13         0.625  \n",
      "14         0.500  \n",
      "15         0.625  \n",
      "16         0.625  \n",
      "17         0.500  \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from gensim.models.poincare import PoincareModel\n",
    "\n",
    "\n",
    "def prepare_custom_data(\n",
    "    tsv_path,\n",
    "    hypernym_col=\"Hypernym\",\n",
    "    hyponym_col=\"Hyponym\",\n",
    "    subset_size=10000,\n",
    "    test_size=0.2,\n",
    "    seed=42\n",
    "):\n",
    "\n",
    "    df = pd.read_csv(tsv_path, sep=\"\\t\", header=0, names=[hypernym_col, hyponym_col])\n",
    "\n",
    "\n",
    "    relations_list = []\n",
    "    for i, row in df.iterrows():\n",
    "        hypernym = str(row[hypernym_col]).strip()\n",
    "        hyponym = str(row[hyponym_col]).strip()\n",
    "        if not hypernym or not hyponym or hypernym == \"nan\" or hyponym == \"nan\":\n",
    "            continue\n",
    "        relations_list.append((hyponym, hypernym))\n",
    "\n",
    "    print(f\"Total edges (before subsampling): {len(relations_list)}\")\n",
    "\n",
    "    if subset_size is not None and len(relations_list) > subset_size:\n",
    "        random.seed(seed)\n",
    "        relations_list = random.sample(relations_list, subset_size)\n",
    "        print(f\"Using a SUBSET of {subset_size} edges for faster testing.\")\n",
    "\n",
    "    train_relations, test_relations = train_test_split(\n",
    "        relations_list, test_size=test_size, random_state=seed\n",
    "    )\n",
    "\n",
    "    print(f\"Train relations: {len(train_relations)}\")\n",
    "    print(f\"Test relations : {len(test_relations)}\")\n",
    "\n",
    "    combined_relations = train_relations + test_relations\n",
    "    return train_relations, test_relations, combined_relations\n",
    "\n",
    "\n",
    "def remove_edges(relations, removal_probability=0.1, seed=42):\n",
    "    random.seed(seed)\n",
    "    kept_relations = []\n",
    "    for (child, ancestor) in relations:\n",
    "        if random.random() > removal_probability:\n",
    "            kept_relations.append((child, ancestor))\n",
    "    return kept_relations\n",
    "\n",
    "\n",
    "def train_and_evaluate_poincare(\n",
    "    train_relations,\n",
    "    test_relations,\n",
    "    combined_relations,\n",
    "    embedding_dim=5,\n",
    "    epochs=300,\n",
    "    n_negatives_strict=500,\n",
    "):\n",
    "\n",
    "    model = PoincareModel(\n",
    "        train_data=train_relations,\n",
    "        size=embedding_dim,\n",
    "        negative=10,\n",
    "        burn_in=10\n",
    "    )\n",
    "    model.train(epochs=epochs)\n",
    "\n",
    "    u_to_all_neighbors = defaultdict(set)\n",
    "    for (child, ancestor) in combined_relations:\n",
    "        u_to_all_neighbors[child].add(ancestor)\n",
    "\n",
    "    vocab_nodes = set(model.kv.index_to_key)\n",
    "    vocab_list = list(vocab_nodes)\n",
    "    all_edges_set = set(combined_relations)\n",
    "\n",
    "    def reconstruction_mean_rank_strict_sampled(model, edges, n_negatives=500, seed=1234):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        ranks = []\n",
    "        for (u, v) in edges:\n",
    "            if (u not in model.kv) or (v not in model.kv):\n",
    "                continue\n",
    "            neighbors = u_to_all_neighbors[u]\n",
    "            neg_candidates = []\n",
    "            attempts = 0\n",
    "            while len(neg_candidates) < n_negatives and attempts < 10000:\n",
    "                candidate = rng.choice(vocab_list)\n",
    "                if candidate not in (u, v) and (candidate not in neighbors):\n",
    "                    neg_candidates.append(candidate)\n",
    "                attempts += 1\n",
    "            candidates = neg_candidates + [v]\n",
    "            dists = [(c, model.kv.distance(u, c)) for c in candidates]\n",
    "            sorted_nodes = [x[0] for x in sorted(dists, key=lambda x: x[1])]\n",
    "            try:\n",
    "                rank = sorted_nodes.index(v) + 1\n",
    "                ranks.append(rank)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return float(np.mean(ranks)) if ranks else 0.0\n",
    "\n",
    "    def reconstruction_map_strict_sampled(model, edges, n_negatives=500, seed=1234):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        reciprocal_ranks = []\n",
    "        for (u, v) in edges:\n",
    "            if (u not in model.kv) or (v not in model.kv):\n",
    "                continue\n",
    "            neighbors = u_to_all_neighbors[u]\n",
    "            neg_candidates = []\n",
    "            attempts = 0\n",
    "            while len(neg_candidates) < n_negatives and attempts < 10000:\n",
    "                candidate = rng.choice(vocab_list)\n",
    "                if candidate not in (u, v) and (candidate not in neighbors):\n",
    "                    neg_candidates.append(candidate)\n",
    "                attempts += 1\n",
    "            candidates = neg_candidates + [v]\n",
    "            dists = [(c, model.kv.distance(u, c)) for c in candidates]\n",
    "            sorted_nodes = [x[0] for x in sorted(dists, key=lambda x: x[1])]\n",
    "            try:\n",
    "                r = sorted_nodes.index(v) + 1\n",
    "                reciprocal_ranks.append(1.0 / r)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return float(np.mean(reciprocal_ranks)) if reciprocal_ranks else 0.0\n",
    "\n",
    "    def link_prediction_mean_rank(model, edges, num_negatives=50):\n",
    "        rng = np.random.default_rng(1234)\n",
    "        ranks = []\n",
    "        for (source, target) in edges:\n",
    "            if source not in vocab_nodes or target not in vocab_nodes:\n",
    "                continue\n",
    "            neg_candidates = []\n",
    "            attempts = 0\n",
    "            while len(neg_candidates) < num_negatives and attempts < 10000:\n",
    "                candidate = rng.choice(vocab_list)\n",
    "                # Must not be the true target, nor an existing edge from (source)\n",
    "                if candidate != target and (source, candidate) not in all_edges_set:\n",
    "                    neg_candidates.append(candidate)\n",
    "                attempts += 1\n",
    "            if not neg_candidates:\n",
    "                continue\n",
    "            candidates = neg_candidates + [target]\n",
    "            dists = [model.kv.distance(source, c) for c in candidates]\n",
    "            sorted_candidates = [c for _, c in sorted(zip(dists, candidates), key=lambda x: x[0])]\n",
    "            rank = sorted_candidates.index(target) + 1\n",
    "            ranks.append(rank)\n",
    "        return float(np.mean(ranks)) if ranks else 0.0\n",
    "\n",
    "    def link_prediction_map(model, edges, num_negatives=50):\n",
    "        rng = np.random.default_rng(1234)\n",
    "        reciprocal_ranks = []\n",
    "        for (source, target) in edges:\n",
    "            if source not in vocab_nodes or target not in vocab_nodes:\n",
    "                continue\n",
    "            neg_candidates = []\n",
    "            attempts = 0\n",
    "            while len(neg_candidates) < num_negatives and attempts < 10000:\n",
    "                candidate = rng.choice(vocab_list)\n",
    "                if candidate != target and (source, candidate) not in all_edges_set:\n",
    "                    neg_candidates.append(candidate)\n",
    "                attempts += 1\n",
    "            if not neg_candidates:\n",
    "                continue\n",
    "            candidates = neg_candidates + [target]\n",
    "            dists = [model.kv.distance(source, c) for c in candidates]\n",
    "            sorted_candidates = [c for _, c in sorted(zip(dists, candidates), key=lambda x: x[0])]\n",
    "            rank = sorted_candidates.index(target) + 1\n",
    "            reciprocal_ranks.append(1.0 / rank)\n",
    "        return float(np.mean(reciprocal_ranks)) if reciprocal_ranks else 0.0\n",
    "\n",
    "    def precision_at_k(model, edges, k=10, num_negatives=50):\n",
    "        rng = np.random.default_rng(1234)\n",
    "        hits = 0\n",
    "        count = 0\n",
    "        for (source, target) in edges:\n",
    "            if source not in vocab_nodes or target not in vocab_nodes:\n",
    "                continue\n",
    "            neg_candidates = []\n",
    "            attempts = 0\n",
    "            while len(neg_candidates) < num_negatives and attempts < 10000:\n",
    "                candidate = rng.choice(vocab_list)\n",
    "                if candidate != target and (source, candidate) not in all_edges_set:\n",
    "                    neg_candidates.append(candidate)\n",
    "                attempts += 1\n",
    "            if not neg_candidates:\n",
    "                continue\n",
    "            candidates = neg_candidates + [target]\n",
    "            dists = [model.kv.distance(source, c) for c in candidates]\n",
    "            sorted_candidates = [c for _, c in sorted(zip(dists, candidates), key=lambda x: x[0])]\n",
    "            top_k_nodes = sorted_candidates[:k]\n",
    "            if target in top_k_nodes:\n",
    "                hits += 1\n",
    "            count += 1\n",
    "        return hits / count if count else 0.0\n",
    "\n",
    "    def recall_at_k(model, edges, k=10, num_negatives=50):\n",
    "        return precision_at_k(model, edges, k, num_negatives)\n",
    "\n",
    "\n",
    "    recon_mr_strict = reconstruction_mean_rank_strict_sampled(\n",
    "        model, train_relations, n_negatives=n_negatives_strict, seed=42\n",
    "    )\n",
    "    recon_map_strict = reconstruction_map_strict_sampled(\n",
    "        model, train_relations, n_negatives=n_negatives_strict, seed=42\n",
    "    )\n",
    "\n",
    "\n",
    "    lp_mr = link_prediction_mean_rank(model, test_relations, num_negatives=50)\n",
    "    lp_map_ = link_prediction_map(model, test_relations, num_negatives=50)\n",
    "    lp_p10 = precision_at_k(model, test_relations, k=10, num_negatives=50)\n",
    "    lp_r10 = recall_at_k(model, test_relations, k=10, num_negatives=50)\n",
    "\n",
    "    return {\n",
    "        \"reconstruction_mean_rank_strict\": recon_mr_strict,\n",
    "        \"reconstruction_map_strict\": recon_map_strict,\n",
    "        \"lp_mean_rank\": lp_mr,\n",
    "        \"lp_map\": lp_map_,\n",
    "        \"lp_precision_10\": lp_p10,\n",
    "        \"lp_recall_10\": lp_r10,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_multiple_experiments(\n",
    "    address,                  \n",
    "    dim_list,                 \n",
    "    removal_prob_list,        \n",
    "    tsv_path,                 \n",
    "    subset_size=10000,\n",
    "    test_size=0.2,\n",
    "    epochs=300,\n",
    "    n_negatives_strict=500,\n",
    "    seed=42\n",
    "):\n",
    "    train_relations_clean, test_relations, combined_relations = prepare_custom_data(\n",
    "        tsv_path=tsv_path,\n",
    "        hypernym_col=\"Hypernym\",\n",
    "        hyponym_col=\"Hyponym\",\n",
    "        subset_size=subset_size,\n",
    "        test_size=test_size,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "\n",
    "    for embedding_dim in dim_list:\n",
    "        for removal_prob in removal_prob_list:\n",
    "            print(\"========================================================\")\n",
    "            print(f\"RUNNING: dim={embedding_dim}, removal_prob={removal_prob:.2f}\")\n",
    "            print(\"========================================================\")\n",
    "\n",
    "  \n",
    "            modified_train = remove_edges(\n",
    "                train_relations_clean,\n",
    "                removal_probability=removal_prob,\n",
    "                seed=seed\n",
    "            )\n",
    "            removed_edges = len(train_relations_clean) - len(modified_train)\n",
    "            print(f\"Removed {removed_edges} edges from training set.\")\n",
    "\n",
    "\n",
    "            run_results = train_and_evaluate_poincare(\n",
    "                train_relations=modified_train,\n",
    "                test_relations=test_relations,\n",
    "                combined_relations=combined_relations,\n",
    "                embedding_dim=embedding_dim,\n",
    "                epochs=epochs,\n",
    "                n_negatives_strict=n_negatives_strict\n",
    "            )\n",
    "\n",
    "\n",
    "            row_info = {\n",
    "                \"embedding_dim\": embedding_dim,\n",
    "                \"edge_removal_probability\": removal_prob,\n",
    "                \"subset_size\": subset_size,\n",
    "                \"test_size\": test_size,\n",
    "                \"epochs\": epochs,\n",
    "                \"n_negatives_strict\": n_negatives_strict,\n",
    "            }\n",
    "            row_info.update(run_results)  \n",
    "            all_results.append(row_info)\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_excel(address, index=False)\n",
    "    print(f\"\\nAll experiment results saved to: {address}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tsv_path = r\"Y:\\Data Science Readings\\Geometry of Information\\hebrew\\hebewnet.tsv\"\n",
    "    dimension_list = [5, 10,20,50,100,200]            \n",
    "    edge_removal_probability_list = [0.0, 0.1, 0.3]  \n",
    "    excel_path = \"Y:\\\\Data Science Readings\\\\Geometry of Information\\\\hebrew\\\\hebrewnet_experiment.xlsx\"\n",
    "    final_df = run_multiple_experiments(\n",
    "        address=excel_path,\n",
    "        dim_list=dimension_list,\n",
    "        removal_prob_list=edge_removal_probability_list,  \n",
    "        tsv_path=tsv_path,        \n",
    "        subset_size=10000,        \n",
    "        test_size=0.2,\n",
    "        epochs=100,                \n",
    "        n_negatives_strict=20,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    print(\"\\nFinal DataFrame of all runs:\\n\")\n",
    "    print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaca793-8f31-4a1b-aa6d-ce5cba738ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
