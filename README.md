# PoincarÃ© Embeddings â€“ Experiments and Extensions
This project explores the performance and robustness of PoincarÃ© embeddings, a type of hyperbolic embedding used to capture hierarchical structures in data. The work is divided into two main phases:

ğŸ”¬ Phase 1: Reproducing and Stress-Testing PoincarÃ© Embeddings
I replicated the core experiments from the original PoincarÃ© paper and applied various "stress tests" to assess embedding performance across different datasets and conditions. The evaluation focused on link prediction and reconstruction, using Mean Rank (MR) and Mean Average Precision (MAP) as metrics.

Experiment 1: Mammals WordNet with edge flipping noise

Experiment 2: Facebook Social Circles with subset size variation

Experiment 3: HebrewNet (custom dataset) with probabilistic edge removal

Across all experiments, I varied embedding dimensions (5 to 200) and evaluated how noise, data size, and dimensionality affected embedding quality.

ğŸ§ª Phase 2: Trainable Curvature Extension
In this phase, I modified the original PoincarÃ© embedding algorithm to allow trainable curvature, replacing the fixed 
ğ‘˜
=
âˆ’
1
k=âˆ’1 assumption with a learnable parameter 
ğ‘˜
=
âˆ’
exp
â¡
(
ğ›¼
)
k=âˆ’exp(Î±). This aimed to make the embedding space more adaptable to different dataset geometries.

Implemented curvature reparameterization with gradient descent

Evaluated performance against original PoincarÃ© model

Found that the original fixed-curvature model consistently outperformed the flexible version across most settings

ğŸ“¦ What's Included
Full experiment results and metrics across all settings

Code for embedding training and curvature modification

Custom dataset (HebrewNet) and data processing scripts

Jupyter notebooks comparing original vs. modified models

ğŸ“‰ Key Insights
The original PoincarÃ© model with 
ğ‘˜
=
âˆ’
1
k=âˆ’1 is surprisingly robustâ€”even for non-hierarchical data

Introducing trainable curvature adds complexity and didnâ€™t yield consistent improvements

Dimensionality matters more under noisy or data-scarce conditions

Link prediction is more sensitive to noise than reconstruction

ğŸ’¡ Future Work
Improving the trainable curvature version may require:

Smarter optimization schedules

Better initialization of curvature

Regularization or clamping strategies

