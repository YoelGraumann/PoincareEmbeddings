{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "879c7df1-1ea3-4cef-83b6-1e7a2cd9bb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=5000, dim=5\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 5000 edges.\n",
      "[INFO] Train relations: 4000\n",
      "[INFO] Test relations : 1000\n",
      "[INFO] Training Poincaré (dim=5, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.24290946363049776, kappa=-0.7843425220814861\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=5000, dim=10\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 5000 edges.\n",
      "[INFO] Train relations: 4000\n",
      "[INFO] Test relations : 1000\n",
      "[INFO] Training Poincaré (dim=10, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-4.1555341554606064, kappa=-0.015677414715987943\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=5000, dim=20\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 5000 edges.\n",
      "[INFO] Train relations: 4000\n",
      "[INFO] Test relations : 1000\n",
      "[INFO] Training Poincaré (dim=20, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.2046283061091212, kappa=-0.8149501721234677\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=5000, dim=50\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 5000 edges.\n",
      "[INFO] Train relations: 4000\n",
      "[INFO] Test relations : 1000\n",
      "[INFO] Training Poincaré (dim=50, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.20503620575556736, kappa=-0.8146178220237394\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=5000, dim=100\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 5000 edges.\n",
      "[INFO] Train relations: 4000\n",
      "[INFO] Test relations : 1000\n",
      "[INFO] Training Poincaré (dim=100, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.2232281435725461, kappa=-0.7999323290555903\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=5000, dim=200\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 5000 edges.\n",
      "[INFO] Train relations: 4000\n",
      "[INFO] Test relations : 1000\n",
      "[INFO] Training Poincaré (dim=200, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.22683756461640206, kappa=-0.7970502409371559\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=10000, dim=5\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 10000 edges.\n",
      "[INFO] Train relations: 8000\n",
      "[INFO] Test relations : 2000\n",
      "[INFO] Training Poincaré (dim=5, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.2458888822202535, kappa=-0.782009115214484\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=10000, dim=10\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 10000 edges.\n",
      "[INFO] Train relations: 8000\n",
      "[INFO] Test relations : 2000\n",
      "[INFO] Training Poincaré (dim=10, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.2221456188633549, kappa=-0.8007987444408956\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=10000, dim=20\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 10000 edges.\n",
      "[INFO] Train relations: 8000\n",
      "[INFO] Test relations : 2000\n",
      "[INFO] Training Poincaré (dim=20, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.23015266676991902, kappa=-0.7944123128833306\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=10000, dim=50\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 10000 edges.\n",
      "[INFO] Train relations: 8000\n",
      "[INFO] Test relations : 2000\n",
      "[INFO] Training Poincaré (dim=50, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.2049278793559146, kappa=-0.8147060714192756\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=10000, dim=100\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 10000 edges.\n",
      "[INFO] Train relations: 8000\n",
      "[INFO] Test relations : 2000\n",
      "[INFO] Training Poincaré (dim=100, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.24243907437452042, kappa=-0.7847115551646736\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=10000, dim=200\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 10000 edges.\n",
      "[INFO] Train relations: 8000\n",
      "[INFO] Test relations : 2000\n",
      "[INFO] Training Poincaré (dim=200, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.23360258929885616, kappa=-0.7916763740499457\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=15000, dim=5\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 15000 edges.\n",
      "[INFO] Train relations: 12000\n",
      "[INFO] Test relations : 3000\n",
      "[INFO] Training Poincaré (dim=5, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.18169874297074898, kappa=-0.8338525065100048\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=15000, dim=10\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 15000 edges.\n",
      "[INFO] Train relations: 12000\n",
      "[INFO] Test relations : 3000\n",
      "[INFO] Training Poincaré (dim=10, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.22744962802233107, kappa=-0.7965625449176599\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=15000, dim=20\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 15000 edges.\n",
      "[INFO] Train relations: 12000\n",
      "[INFO] Test relations : 3000\n",
      "[INFO] Training Poincaré (dim=20, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.22517324963184787, kappa=-0.7983778881016662\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=15000, dim=50\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 15000 edges.\n",
      "[INFO] Train relations: 12000\n",
      "[INFO] Test relations : 3000\n",
      "[INFO] Training Poincaré (dim=50, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.23031048168521653, kappa=-0.794286952663578\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=15000, dim=100\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 15000 edges.\n",
      "[INFO] Train relations: 12000\n",
      "[INFO] Test relations : 3000\n",
      "[INFO] Training Poincaré (dim=100, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.21976228032642578, kappa=-0.802709595146391\n",
      "========================================================\n",
      "RUNNING EXPERIMENT: subset_size=15000, dim=200\n",
      "========================================================\n",
      "[INFO] Loading edges from: /Users/alineduthilleul/Desktop/facebook_finaldata.txt ...\n",
      "[INFO] Total edges loaded: 88234\n",
      "[INFO] Using a SUBSET of 15000 edges.\n",
      "[INFO] Train relations: 12000\n",
      "[INFO] Test relations : 3000\n",
      "[INFO] Training Poincaré (dim=200, epochs=100) ...\n",
      "[INFO] Reparam kappa => alpha_init=0.0, lr_alpha=0.001, alpha_emb=0.1\n",
      "[INFO] Final alpha=-0.23435332982595983, kappa=-0.7910822535546673\n",
      "\n",
      "[INFO] All experiment results saved to '/Users/alineduthilleul/Desktop/facebook_experimentsM3.xlsx'\n",
      "\n",
      "All experiment runs:\n",
      "     subset_size  embedding_dim  test_size  epochs  n_negatives_strict  \\\n",
      "0          5000              5        0.2     100                  50   \n",
      "1          5000             10        0.2     100                  50   \n",
      "2          5000             20        0.2     100                  50   \n",
      "3          5000             50        0.2     100                  50   \n",
      "4          5000            100        0.2     100                  50   \n",
      "5          5000            200        0.2     100                  50   \n",
      "6         10000              5        0.2     100                  50   \n",
      "7         10000             10        0.2     100                  50   \n",
      "8         10000             20        0.2     100                  50   \n",
      "9         10000             50        0.2     100                  50   \n",
      "10        10000            100        0.2     100                  50   \n",
      "11        10000            200        0.2     100                  50   \n",
      "12        15000              5        0.2     100                  50   \n",
      "13        15000             10        0.2     100                  50   \n",
      "14        15000             20        0.2     100                  50   \n",
      "15        15000             50        0.2     100                  50   \n",
      "16        15000            100        0.2     100                  50   \n",
      "17        15000            200        0.2     100                  50   \n",
      "\n",
      "    recon_mean_rank_strict  recon_map_strict  lp_mean_rank    lp_map  \\\n",
      "0                 5.613250          0.552859     11.056774  0.330419   \n",
      "1                 1.936250          0.735075      4.941935  0.485642   \n",
      "2                 5.394750          0.639245     11.209032  0.370397   \n",
      "3                 4.895250          0.659650     11.581935  0.357258   \n",
      "4                 5.198750          0.651556     11.676129  0.370890   \n",
      "5                 5.132500          0.651494     11.588387  0.355989   \n",
      "6                 5.916625          0.502652      8.308880  0.405599   \n",
      "7                 5.244250          0.562745      7.722559  0.435692   \n",
      "8                 4.969250          0.580680      7.509653  0.469723   \n",
      "9                 4.707875          0.594534      7.235521  0.467617   \n",
      "10                4.687375          0.590252      7.252620  0.468357   \n",
      "11                4.971375          0.590458      7.111418  0.466297   \n",
      "12                6.016333          0.494750      7.514665  0.431954   \n",
      "13                5.525417          0.535506      7.074372  0.463389   \n",
      "14                4.910667          0.561615      6.663059  0.476238   \n",
      "15                5.534417          0.545586      7.155028  0.467955   \n",
      "16                5.173083          0.554826      6.996858  0.462885   \n",
      "17                4.706000          0.564032      6.366620  0.488939   \n",
      "\n",
      "    lp_precision_10  lp_recall_10  \n",
      "0          0.656774      0.656774  \n",
      "1          0.920000      0.920000  \n",
      "2          0.642581      0.642581  \n",
      "3          0.649032      0.649032  \n",
      "4          0.640000      0.640000  \n",
      "5          0.630968      0.630968  \n",
      "6          0.766685      0.766685  \n",
      "7          0.786542      0.786542  \n",
      "8          0.801986      0.801986  \n",
      "9          0.806950      0.806950  \n",
      "10         0.815775      0.815775  \n",
      "11         0.815223      0.815223  \n",
      "12         0.794693      0.794693  \n",
      "13         0.813547      0.813547  \n",
      "14         0.832402      0.832402  \n",
      "15         0.810754      0.810754  \n",
      "16         0.816341      0.816341  \n",
      "17         0.842179      0.842179  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import autograd\n",
    "from autograd import grad\n",
    "import autograd.numpy as anp  \n",
    "from types import MethodType\n",
    "\n",
    "from gensim.models.poincare import (\n",
    "    PoincareKeyedVectors,\n",
    "    PoincareModel,\n",
    "    PoincareBatch\n",
    ")\n",
    "\n",
    "def no_op_compute_all(self):\n",
    "    self.loss = 0.0\n",
    "\n",
    "PoincareBatch.compute_all = no_op_compute_all\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def anp_safe_norm(x, axis=None, eps=1e-9):\n",
    "    return anp.sqrt(anp.sum(x * x, axis=axis) + eps)\n",
    "\n",
    "def anp_safe_arccosh(x, eps=1e-9):\n",
    "    clipped_x = anp.clip(x, 1.0 + eps, 1e15)  \n",
    "    inside = clipped_x * clipped_x - 1.0\n",
    "    return anp.log(clipped_x + anp.sqrt(inside + eps))\n",
    "\n",
    "\n",
    "\n",
    "def custom_loss_with_alpha(matrix, alpha, regularization_coeff=1.0):\n",
    "    kappa = -anp.exp(alpha)  \n",
    "    vector_u = matrix[0]\n",
    "    vectors_v = matrix[1:]  \n",
    "    eucl_dists = anp_safe_norm(vector_u - vectors_v, axis=1)\n",
    "    norm_u = anp_safe_norm(vector_u)\n",
    "    norms_v = anp_safe_norm(vectors_v, axis=1)\n",
    "    denom_u = (1.0 + kappa * (norm_u**2))\n",
    "    denom_v = (1.0 + kappa * (norms_v**2))\n",
    "    denom = denom_u * denom_v\n",
    "    denom = anp.clip(denom, 1e-9, 1e15)  \n",
    "    gamma = 1.0 + 2.0 * (eucl_dists**2) / denom\n",
    "    gamma = anp.clip(gamma, 1.0 + 1e-9, 1e15)\n",
    "    poincare_dists = anp_safe_arccosh(gamma)\n",
    "    exp_neg = anp.exp(-poincare_dists)\n",
    "    reg_term = regularization_coeff * anp.sum(vectors_v[0] * vectors_v[0])\n",
    "    return -anp.log(exp_neg[0] / anp.sum(exp_neg)) + reg_term\n",
    "\n",
    "\n",
    "\n",
    "_loss_grad_wrt_both = grad(custom_loss_with_alpha, argnum=[0, 1])\n",
    "\n",
    "\n",
    "\n",
    "def monkey_patch_train_on_batch_with_alpha(self, relations, check_gradients=False):\n",
    "    all_negatives = self._sample_negatives_batch(r[0] for r in relations)\n",
    "    batch = self._prepare_training_batch(relations, all_negatives, check_gradients)\n",
    "    total_loss = 0.0\n",
    "    for (u, v), negs in zip(relations, all_negatives):\n",
    "        vec_u = self.kv.vectors[u]\n",
    "        vec_v = self.kv.vectors[v]\n",
    "        vec_negs = self.kv.vectors[negs]\n",
    "        sub_matrix = np.vstack((vec_u, vec_v, vec_negs))\n",
    "        grad_matrix, grad_alpha = _loss_grad_wrt_both(\n",
    "            sub_matrix, self.alpha, self.regularization_coeff\n",
    "        )\n",
    "        self.kv.vectors[u] -= self.alpha_emb * grad_matrix[0]\n",
    "        self.kv.vectors[v] -= self.alpha_emb * grad_matrix[1]\n",
    "        for i, neg_idx in enumerate(negs):\n",
    "            self.kv.vectors[neg_idx] -= self.alpha_emb * grad_matrix[2 + i]\n",
    "        self.kv.vectors[u] = self._clip_vectors(self.kv.vectors[u], self.epsilon)\n",
    "        self.kv.vectors[v] = self._clip_vectors(self.kv.vectors[v], self.epsilon)\n",
    "        self.kv.vectors[negs] = self._clip_vectors(self.kv.vectors[negs], self.epsilon)\n",
    "        self.alpha -= self.lr_alpha * grad_alpha\n",
    "        example_loss = float(custom_loss_with_alpha(sub_matrix, self.alpha, self.regularization_coeff))\n",
    "        total_loss += example_loss\n",
    "\n",
    "    batch.loss = total_loss / len(relations)\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "def set_learnable_kappa_reparam(model, alpha_init=0.0, lr_alpha=1e-3, alpha_emb=0.1):\n",
    "    model.alpha = alpha_init\n",
    "    model.lr_alpha = lr_alpha\n",
    "    model.alpha_emb = alpha_emb  \n",
    "    model._train_on_batch = MethodType(monkey_patch_train_on_batch_with_alpha, model)\n",
    "\n",
    "    print(f\"[INFO] Reparam kappa => alpha_init={alpha_init}, lr_alpha={lr_alpha}, alpha_emb={alpha_emb}\")\n",
    "\n",
    "\n",
    "def curved_poincare_distance(u, v, kappa):\n",
    "    eucl_dist = np.linalg.norm(u - v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    denom = (1.0 + kappa * norm_u**2) * (1.0 + kappa * norm_v**2)\n",
    "    if denom <= 0:\n",
    "        denom = 1e-15\n",
    "    gamma = 1.0 + 2.0 * (eucl_dist**2) / denom\n",
    "    if gamma < 1.0:\n",
    "        gamma = 1.0\n",
    "    return np.arccosh(gamma)\n",
    "\n",
    "def custom_vector_distance(self, u, v):\n",
    "    alpha = getattr(self, \"model_alpha\", 0.0)\n",
    "    kappa = -np.exp(alpha)  \n",
    "    return curved_poincare_distance(u, v, kappa)\n",
    "\n",
    "def custom_vector_distance_batch(self, u, all_v):\n",
    "    alpha = getattr(self, \"model_alpha\", 0.0)\n",
    "    kappa = -np.exp(alpha)\n",
    "    euc_dists = np.linalg.norm(u - all_v, axis=1)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norms_v = np.linalg.norm(all_v, axis=1)\n",
    "    denom = (1.0 + kappa * norm_u**2) * (1.0 + kappa * (norms_v**2))\n",
    "    denom[denom <= 0] = 1e-15\n",
    "    gamma = 1.0 + 2.0 * (euc_dists**2) / denom\n",
    "    gamma[gamma < 1.0] = 1.0\n",
    "    return np.arccosh(gamma)\n",
    "\n",
    "PoincareKeyedVectors.vector_distance = custom_vector_distance\n",
    "PoincareKeyedVectors.vector_distance_batch = custom_vector_distance_batch\n",
    "\n",
    "def no_op_loss_fn(*args, **kwargs):\n",
    "    return 0.0\n",
    "PoincareModel._loss_fn = no_op_loss_fn\n",
    "\n",
    "\n",
    "def load_facebook_edges(filepath, subset_size=None, test_size=0.2, seed=42):\n",
    "    random.seed(seed)\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Edge list file not found: {filepath}\")\n",
    "\n",
    "    print(f\"[INFO] Loading edges from: {filepath} ...\")\n",
    "    edges = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            u, v = parts[0], parts[1]\n",
    "            edges.append((u, v))\n",
    "    print(f\"[INFO] Total edges loaded: {len(edges)}\")\n",
    "\n",
    "    if subset_size is not None and len(edges) > subset_size:\n",
    "        edges = random.sample(edges, subset_size)\n",
    "        print(f\"[INFO] Using a SUBSET of {subset_size} edges.\")\n",
    "\n",
    "    train_rel, test_rel = train_test_split(edges, test_size=test_size, random_state=seed)\n",
    "    print(f\"[INFO] Train relations: {len(train_rel)}\")\n",
    "    print(f\"[INFO] Test relations : {len(test_rel)}\")\n",
    "    combined_rel = train_rel + test_rel\n",
    "    return train_rel, test_rel, combined_rel\n",
    "\n",
    "\n",
    "from gensim.models.poincare import PoincareModel\n",
    "def train_and_evaluate_poincare(\n",
    "    train_relations,\n",
    "    test_relations,\n",
    "    combined_relations,\n",
    "    embedding_dim=10,\n",
    "    epochs=50,\n",
    "    n_negatives_strict=500\n",
    "):\n",
    "    print(f\"[INFO] Training Poincaré (dim={embedding_dim}, epochs={epochs}) ...\")\n",
    "    model = PoincareModel(\n",
    "        train_data=train_relations,\n",
    "        size=embedding_dim,\n",
    "        negative=10,\n",
    "        burn_in=10\n",
    "    )\n",
    "\n",
    "\n",
    "    set_learnable_kappa_reparam(model, alpha_init=0.0, lr_alpha=1e-3, alpha_emb=0.1)\n",
    "\n",
    "\n",
    "    model.train(epochs=epochs)\n",
    "\n",
    "    final_alpha = model.alpha\n",
    "    final_kappa = -np.exp(final_alpha)\n",
    "    print(f\"[INFO] Final alpha={final_alpha}, kappa={final_kappa}\")\n",
    "\n",
    "    model.kv.model_alpha = final_alpha\n",
    "\n",
    "    u_to_all_neighbors = defaultdict(set)\n",
    "    for (u, v) in combined_relations:\n",
    "        u_to_all_neighbors[u].add(v)\n",
    "    vocab_nodes = set(model.kv.index_to_key)\n",
    "    vocab_list = list(vocab_nodes)\n",
    "    all_edges_set = set(combined_relations)\n",
    "\n",
    "    recon_mr_strict = reconstruction_mean_rank_strict_sampled(\n",
    "        model, train_relations, u_to_all_neighbors, vocab_list, n_negatives_strict\n",
    "    )\n",
    "    recon_map_strict = reconstruction_map_strict_sampled(\n",
    "        model, train_relations, u_to_all_neighbors, vocab_list, n_negatives_strict\n",
    "    )\n",
    "    lp_mr = link_prediction_mean_rank(model, test_relations, vocab_list, all_edges_set)\n",
    "    lp_map_ = link_prediction_map(model, test_relations, vocab_list, all_edges_set)\n",
    "    lp_p10 = precision_at_k(model, test_relations, vocab_list, all_edges_set, k=10)\n",
    "    lp_r10 = recall_at_k(model, test_relations, vocab_list, all_edges_set, k=10)\n",
    "\n",
    "    return {\n",
    "        \"recon_mean_rank_strict\": recon_mr_strict,\n",
    "        \"recon_map_strict\": recon_map_strict,\n",
    "        \"lp_mean_rank\": lp_mr,\n",
    "        \"lp_map\": lp_map_,\n",
    "        \"lp_precision_10\": lp_p10,\n",
    "        \"lp_recall_10\": lp_r10,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def reconstruction_mean_rank_strict_sampled(model, edges, u_to_all_neighbors, vocab_list, n_negatives=500, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    ranks = []\n",
    "    for (u, v) in edges:\n",
    "        if (u not in model.kv) or (v not in model.kv):\n",
    "            continue\n",
    "        neighbors = u_to_all_neighbors[u]\n",
    "        neg_candidates = []\n",
    "        attempts = 0\n",
    "        while len(neg_candidates) < n_negatives and attempts < 10000:\n",
    "            candidate = rng.choice(vocab_list)\n",
    "            if candidate not in (u, v) and candidate not in neighbors:\n",
    "                neg_candidates.append(candidate)\n",
    "            attempts += 1\n",
    "        candidates = neg_candidates + [v]\n",
    "        dists = [(c, model.kv.distance(u, c)) for c in candidates]\n",
    "        sorted_nodes = [x[0] for x in sorted(dists, key=lambda x: x[1])]\n",
    "        try:\n",
    "            rank = sorted_nodes.index(v) + 1\n",
    "            ranks.append(rank)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return float(np.mean(ranks)) if ranks else 0.0\n",
    "\n",
    "def reconstruction_map_strict_sampled(model, edges, u_to_all_neighbors, vocab_list, n_negatives=500, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    reciprocal_ranks = []\n",
    "    for (u, v) in edges:\n",
    "        if (u not in model.kv) or (v not in model.kv):\n",
    "            continue\n",
    "        neighbors = u_to_all_neighbors[u]\n",
    "        neg_candidates = []\n",
    "        attempts = 0\n",
    "        while len(neg_candidates) < n_negatives and attempts < 10000:\n",
    "            candidate = rng.choice(vocab_list)\n",
    "            if candidate not in (u, v) and candidate not in neighbors:\n",
    "                neg_candidates.append(candidate)\n",
    "            attempts += 1\n",
    "        candidates = neg_candidates + [v]\n",
    "        dists = [(c, model.kv.distance(u, c)) for c in candidates]\n",
    "        sorted_nodes = [x[0] for x in sorted(dists, key=lambda x: x[1])]\n",
    "        try:\n",
    "            r = sorted_nodes.index(v) + 1\n",
    "            reciprocal_ranks.append(1.0 / r)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return float(np.mean(reciprocal_ranks)) if reciprocal_ranks else 0.0\n",
    "\n",
    "def link_prediction_mean_rank(model, edges, vocab_list, all_edges_set, num_negatives=50):\n",
    "    rng = np.random.default_rng(1234)\n",
    "    ranks = []\n",
    "    for (source, target) in edges:\n",
    "        if (source not in model.kv) or (target not in model.kv):\n",
    "            continue\n",
    "        neg_candidates = []\n",
    "        attempts = 0\n",
    "        while len(neg_candidates) < num_negatives and attempts < 10000:\n",
    "            candidate = rng.choice(vocab_list)\n",
    "            if candidate != target and (source, candidate) not in all_edges_set:\n",
    "                neg_candidates.append(candidate)\n",
    "            attempts += 1\n",
    "        if not neg_candidates:\n",
    "            continue\n",
    "        candidates = neg_candidates + [target]\n",
    "        dists = [model.kv.distance(source, c) for c in candidates]\n",
    "        sorted_candidates = [c for _, c in sorted(zip(dists, candidates), key=lambda x: x[0])]\n",
    "        rank = sorted_candidates.index(target) + 1\n",
    "        ranks.append(rank)\n",
    "    return float(np.mean(ranks)) if ranks else 0.0\n",
    "\n",
    "def link_prediction_map(model, edges, vocab_list, all_edges_set, num_negatives=50):\n",
    "    rng = np.random.default_rng(1234)\n",
    "    reciprocal_ranks = []\n",
    "    for (source, target) in edges:\n",
    "        if (source not in model.kv) or (target not in model.kv):\n",
    "            continue\n",
    "        neg_candidates = []\n",
    "        attempts = 0\n",
    "        while len(neg_candidates) < num_negatives and attempts < 10000:\n",
    "            candidate = rng.choice(vocab_list)\n",
    "            if candidate != target and (source, candidate) not in all_edges_set:\n",
    "                neg_candidates.append(candidate)\n",
    "            attempts += 1\n",
    "        if not neg_candidates:\n",
    "            continue\n",
    "        candidates = neg_candidates + [target]\n",
    "        dists = [model.kv.distance(source, c) for c in candidates]\n",
    "        sorted_candidates = [c for _, c in sorted(zip(dists, candidates), key=lambda x: x[0])]\n",
    "        rank = sorted_candidates.index(target) + 1\n",
    "        reciprocal_ranks.append(1.0 / rank)\n",
    "    return float(np.mean(reciprocal_ranks)) if reciprocal_ranks else 0.0\n",
    "\n",
    "def precision_at_k(model, edges, vocab_list, all_edges_set, k=10, num_negatives=50):\n",
    "    rng = np.random.default_rng(1234)\n",
    "    hits = 0\n",
    "    count = 0\n",
    "    for (source, target) in edges:\n",
    "        if (source not in model.kv) or (target not in model.kv):\n",
    "            continue\n",
    "        neg_candidates = []\n",
    "        attempts = 0\n",
    "        while len(neg_candidates) < num_negatives and attempts < 10000:\n",
    "            candidate = rng.choice(vocab_list)\n",
    "            if candidate != target and (source, candidate) not in all_edges_set:\n",
    "                neg_candidates.append(candidate)\n",
    "            attempts += 1\n",
    "        if not neg_candidates:\n",
    "            continue\n",
    "        candidates = neg_candidates + [target]\n",
    "        dists = [model.kv.distance(source, c) for c in candidates]\n",
    "        sorted_candidates = [c for _, c in sorted(zip(dists, candidates), key=lambda x: x[0])]\n",
    "        top_k_nodes = sorted_candidates[:k]\n",
    "        if target in top_k_nodes:\n",
    "            hits += 1\n",
    "        count += 1\n",
    "    return hits / count if count else 0.0\n",
    "\n",
    "def recall_at_k(model, edges, vocab_list, all_edges_set, k=10, num_negatives=50):\n",
    "    return precision_at_k(model, edges, vocab_list, all_edges_set, k, num_negatives)\n",
    "\n",
    "\n",
    "\n",
    "def run_multiple_experiments(\n",
    "    edge_file=\"facebook_edges.txt\",\n",
    "    subset_sizes=[50, 100, 150],\n",
    "    dims=[5, 10, 20, 50, 100, 200],\n",
    "    test_size=0.2,\n",
    "    epochs=100,\n",
    "    n_negatives_strict=50,\n",
    "    excel_output=\"facebook_experiments.xlsx\"\n",
    "):\n",
    "    all_results = []\n",
    "    for s_size in subset_sizes:\n",
    "        for dim in dims:\n",
    "            print(\"========================================================\")\n",
    "            print(f\"RUNNING EXPERIMENT: subset_size={s_size}, dim={dim}\")\n",
    "            print(\"========================================================\")\n",
    "            train_rel, test_rel, combined_rel = load_facebook_edges(\n",
    "                filepath=edge_file,\n",
    "                subset_size=s_size,\n",
    "                test_size=test_size,\n",
    "                seed=42\n",
    "            )\n",
    "            results = train_and_evaluate_poincare(\n",
    "                train_relations=train_rel,\n",
    "                test_relations=test_rel,\n",
    "                combined_relations=combined_rel,\n",
    "                embedding_dim=dim,\n",
    "                epochs=epochs,\n",
    "                n_negatives_strict=n_negatives_strict\n",
    "            )\n",
    "            row_info = {\n",
    "                \"subset_size\": s_size,\n",
    "                \"embedding_dim\": dim,\n",
    "                \"test_size\": test_size,\n",
    "                \"epochs\": epochs,\n",
    "                \"n_negatives_strict\": n_negatives_strict\n",
    "            }\n",
    "            row_info.update(results)\n",
    "            all_results.append(row_info)\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_excel(excel_output, index=False)\n",
    "    print(f\"\\n[INFO] All experiment results saved to '{excel_output}'\")\n",
    "    return df\n",
    "\n",
    "\n",
    "edge_file = os.path.expanduser(\"~/Desktop/facebook_finaldata.txt\")\n",
    "excel_output = os.path.expanduser(\"~/Desktop/facebook_experimentsM3.xlsx\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_multi = run_multiple_experiments(\n",
    "        edge_file=edge_file,\n",
    "        subset_sizes=[5000, 10000, 15000],\n",
    "        dims=[5, 10, 20, 50, 100, 200],\n",
    "        test_size=0.2,\n",
    "        epochs=100,\n",
    "        n_negatives_strict=50,\n",
    "        excel_output=excel_output\n",
    "    )\n",
    "    print(\"\\nAll experiment runs:\\n\", final_multi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c787b-ef66-477a-8d64-b1f392676fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
